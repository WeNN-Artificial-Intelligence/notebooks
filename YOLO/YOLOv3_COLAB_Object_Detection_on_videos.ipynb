{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeNN-Artificial-Intelligence/notebooks/blob/ilker_study/YOLO/COLAB_Object_Detection_on_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WeNN-Artificial-Intelligence/notebooks.git"
      ],
      "metadata": {
        "id": "5NYzleSAa9DI",
        "outputId": "da5eea0f-82e1-4d90-86eb-0bc06ad5c6a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5NYzleSAa9DI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'notebooks'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 79 (delta 30), reused 53 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd notebooks/YOLO/model\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "id": "2Bt6HUeuiXNd",
        "outputId": "00cb1e8e-35e4-4a4f-9f22-631959386440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2Bt6HUeuiXNd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/notebooks/YOLO/model\n",
            "--2022-01-03 20:03:50--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  63.2MB/s    in 4.0s    \n",
            "\n",
            "2022-01-03 20:03:54 (59.5 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls #in model folder\n",
        "%cd ..\n",
        "!ls #in YOLO folder"
      ],
      "metadata": {
        "id": "x8ZJKMtVbExu",
        "outputId": "3097657d-01ea-4f33-80bd-a5d789716e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "x8ZJKMtVbExu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels.txt  photo.jpg  yolov3.cfg  yolov3.weights\n",
            "/content/notebooks/YOLO\n",
            "bioimage.png  Object_detection_on_images.ipynb\n",
            "labels.txt    Object_Detection_on_videos.ipynb\n",
            "model\t      street.jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade96d3f",
      "metadata": {
        "id": "ade96d3f"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc00ac0",
      "metadata": {
        "id": "5bc00ac0"
      },
      "source": [
        "### Read and the split labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bee3f9e",
      "metadata": {
        "id": "9bee3f9e",
        "outputId": "5881361c-013b-4dbe-a21d-116724a58e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'trafficlight', 'firehydrant', 'stopsign', 'parkingmeter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sportsball', 'kite', 'baseballbat', 'baseballglove', 'skateboard', 'surfboard', 'tennisracket', 'bottle', 'wineglass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hotdog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cellphone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddybear', 'hairdrier', 'toothbrush']\n",
            "person\n"
          ]
        }
      ],
      "source": [
        "label_file = open(\"model/labels.txt\", 'r')\n",
        "labels = [word.replace('\"','').replace(\"'\",'') for word in label_file.read().split(',')]\n",
        "label_file.close()\n",
        "print(len(labels),labels)\n",
        "print(labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a422a3",
      "metadata": {
        "id": "e5a422a3"
      },
      "source": [
        "### Create color list for bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde6b1b5",
      "metadata": {
        "id": "dde6b1b5",
        "outputId": "2104317e-446e-4f18-a4e2-f52b70d81679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "colors = [\"0,255,0\",\"0,0,255\",\"255,0,0\",\"0,120,30\",\"0,30,120\",\"50,50,50\",\"50,0,50\",\"50,50,100\"]\n",
        "colors = [np.array(color.split(\",\")).astype(\"int\") for color in colors]\n",
        "colors = np.array(colors)\n",
        "colors = np.tile(colors,(10,1)) #copying color list 10 times to fill the array with same numbers vertically\n",
        "len(colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions that will be used to convert between different image types within our later steps"
      ],
      "metadata": {
        "id": "PVO0Lw5fobYC"
      },
      "id": "PVO0Lw5fobYC"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "KicvSuVky0WQ"
      },
      "id": "KicvSuVky0WQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below is a function to take the webcam picture using JavaScript and then run YOLO on it."
      ],
      "metadata": {
        "id": "OvHdd3emoO6T"
      },
      "id": "OvHdd3emoO6T"
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  #detect objects\n",
        "  detect_objects(img)\n",
        "  #write the labeled image\n",
        "  cv2.imwrite(filename, img)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "3qRZgoFczDR1"
      },
      "id": "3qRZgoFczDR1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "825504d6",
      "metadata": {
        "id": "825504d6"
      },
      "source": [
        "### Read the model and layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d32059",
      "metadata": {
        "id": "f1d32059"
      },
      "outputs": [],
      "source": [
        "model = cv2.dnn.readNetFromDarknet(\"model/yolov3.cfg\",\"model/yolov3.weights\")\n",
        "layers = model.getLayerNames()\n",
        "output_layers = [layers[int(layer)-1] for layer in model.getUnconnectedOutLayers()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for object detection on a frame using YOLO model"
      ],
      "metadata": {
        "id": "LHp-Qj4toDQF"
      },
      "id": "LHp-Qj4toDQF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01739fe4",
      "metadata": {
        "id": "01739fe4"
      },
      "outputs": [],
      "source": [
        "def detect_objects(frame):\n",
        "    frame_width = frame.shape[1]\n",
        "    frame_height = frame.shape[0]\n",
        "\n",
        "    frame_blob = cv2.dnn.blobFromImage(frame, 1/255, (416,416), swapRB=True,crop=False)\n",
        "    model.setInput(frame_blob)\n",
        "    detection_layers = model.forward(output_layers)\n",
        "\n",
        "    ids_list = []\n",
        "    boxes_list = []\n",
        "    confidences_list = []\n",
        "    for detection_layer in detection_layers:\n",
        "        for object_detection in detection_layer:\n",
        "            scores = object_detection[5:]\n",
        "            predicted_id = np.argmax(scores)\n",
        "            confidence = scores[predicted_id]\n",
        "            if confidence > 0.20: #draw bounding box if confidence is higher than ..\n",
        "                label = labels[predicted_id]\n",
        "                bounding_box = object_detection[0:4] * np.array([frame_width,frame_height,frame_width,frame_height])\n",
        "                (box_center_x, box_center_y,box_width, box_height) = bounding_box.astype(\"int\")\n",
        "\n",
        "                start_x = int(box_center_x - (box_width/2))\n",
        "                start_y = int(box_center_y - (box_height/2))\n",
        "\n",
        "                ## non-maximum surpression step 2 ##\n",
        "                ids_list.append(predicted_id)\n",
        "                confidences_list.append(float(confidence))\n",
        "                boxes_list.append([start_x,start_y,int(box_width),int(box_height)])\n",
        "                ## non-maximum surpression step 2 ##\n",
        "\n",
        "    ## non-maximum surpression step 3 ##\n",
        "    max_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, 0.5, 0.4)\n",
        "    for max_id in max_ids:\n",
        "        max_class_id = max_id[0]\n",
        "        box = boxes_list[max_class_id]\n",
        "        start_x = box[0]\n",
        "        start_y = box[1]\n",
        "        box_width = box[2]\n",
        "        box_height = box[3]\n",
        "\n",
        "        predicted_id = ids_list[max_class_id]\n",
        "        label = labels[predicted_id]\n",
        "        confidence = confidences_list[max_class_id]\n",
        "    ## non-maximum surpression step 3 ##\n",
        "\n",
        "        end_x = start_x + box_width\n",
        "        end_y = start_y + box_height\n",
        "\n",
        "        box_color = colors[predicted_id]\n",
        "        box_color = [int(each) for each in box_color]\n",
        "\n",
        "        label = \"{}: {:.2f}%\".format(label,confidence*100)\n",
        "        print(\"predicted object: {}\".format(label))\n",
        "\n",
        "        cv2.rectangle(frame, (start_x, start_y),(end_x, end_y),box_color,2)\n",
        "        cv2.putText(frame, label, (start_x, start_y-10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color,1)\n",
        "    #cv2.imshow(\"Detection window\", frame)\n",
        "    #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Converting BGR to RGB\n",
        "    #display(Image.fromarray(frame))\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing model with image capture from video stream"
      ],
      "metadata": {
        "id": "_OIIh5xfn1JX"
      },
      "id": "_OIIh5xfn1JX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c0e276",
      "metadata": {
        "id": "12c0e276"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "COLAB_Object_Detection_on_videos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
