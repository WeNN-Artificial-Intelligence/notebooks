{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade96d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc00ac0",
   "metadata": {},
   "source": [
    "### Read and the split labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bee3f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'trafficlight', 'firehydrant', 'stopsign', 'parkingmeter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sportsball', 'kite', 'baseballbat', 'baseballglove', 'skateboard', 'surfboard', 'tennisracket', 'bottle', 'wineglass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hotdog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cellphone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddybear', 'hairdrier', 'toothbrush']\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "label_file = open(\"model/labels.txt\", 'r')\n",
    "labels = [word.replace('\"','').replace(\"'\",'') for word in label_file.read().split(',')]\n",
    "label_file.close()\n",
    "print(len(labels),labels)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a422a3",
   "metadata": {},
   "source": [
    "### Create color list for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde6b1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [\"0,255,0\",\"0,0,255\",\"255,0,0\",\"0,120,30\",\"0,30,120\",\"50,50,50\",\"50,0,50\",\"50,50,100\"]\n",
    "colors = [np.array(color.split(\",\")).astype(\"int\") for color in colors]\n",
    "colors = np.array(colors)\n",
    "colors = np.tile(colors,(10,1)) #copying color list 10 times to fill the array with same numbers vertically\n",
    "len(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825504d6",
   "metadata": {},
   "source": [
    "### Read the model and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d32059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv2.dnn.readNetFromDarknet(\"model/yolov3.cfg\",\"model/yolov3.weights\")\n",
    "layers = model.getLayerNames()\n",
    "output_layers = [layers[int(layer)-1] for layer in model.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390b84d",
   "metadata": {},
   "source": [
    "### Stream and detect object on video camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01739fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object: person: 76.65%\n",
      "predicted object: person: 97.93%\n",
      "predicted object: person: 99.99%\n",
      "predicted object: person: 99.98%\n",
      "predicted object: person: 99.96%\n",
      "predicted object: person: 99.77%\n",
      "predicted object: person: 99.46%\n",
      "predicted object: person: 99.66%\n",
      "predicted object: person: 99.87%\n",
      "predicted object: person: 99.82%\n",
      "predicted object: person: 99.84%\n",
      "predicted object: person: 99.90%\n",
      "predicted object: person: 99.90%\n",
      "predicted object: person: 99.71%\n",
      "predicted object: cup: 99.03%\n",
      "predicted object: person: 99.71%\n",
      "predicted object: cup: 88.04%\n",
      "predicted object: cup: 99.64%\n",
      "predicted object: person: 99.59%\n",
      "predicted object: cup: 98.82%\n",
      "predicted object: person: 98.42%\n",
      "predicted object: cup: 98.58%\n",
      "predicted object: person: 86.44%\n",
      "predicted object: cup: 99.52%\n",
      "predicted object: person: 97.54%\n",
      "predicted object: cup: 99.70%\n",
      "predicted object: person: 97.71%\n",
      "predicted object: cup: 99.80%\n",
      "predicted object: person: 99.12%\n",
      "predicted object: person: 99.71%\n",
      "predicted object: cup: 95.85%\n",
      "predicted object: cup: 99.37%\n",
      "predicted object: cellphone: 99.37%\n",
      "predicted object: person: 98.25%\n",
      "predicted object: cellphone: 99.83%\n",
      "predicted object: person: 98.18%\n",
      "predicted object: cup: 92.44%\n",
      "predicted object: cellphone: 99.94%\n",
      "predicted object: person: 97.75%\n",
      "predicted object: cup: 91.30%\n",
      "predicted object: cellphone: 99.75%\n",
      "predicted object: cup: 99.41%\n",
      "predicted object: person: 98.36%\n",
      "predicted object: cup: 98.72%\n",
      "predicted object: person: 97.31%\n",
      "predicted object: bottle: 55.06%\n",
      "predicted object: cup: 99.10%\n",
      "predicted object: person: 87.02%\n",
      "predicted object: cup: 99.15%\n",
      "predicted object: person: 95.02%\n",
      "predicted object: person: 99.76%\n",
      "predicted object: cup: 89.40%\n",
      "predicted object: person: 98.46%\n",
      "predicted object: cup: 97.14%\n",
      "predicted object: person: 99.34%\n",
      "predicted object: cup: 96.65%\n",
      "predicted object: person: 98.26%\n",
      "predicted object: cup: 91.20%\n",
      "predicted object: person: 98.75%\n",
      "predicted object: cup: 96.45%\n",
      "predicted object: person: 98.54%\n",
      "predicted object: cup: 71.16%\n",
      "predicted object: person: 98.25%\n",
      "predicted object: cup: 88.97%\n",
      "predicted object: person: 99.60%\n",
      "predicted object: cup: 94.91%\n",
      "predicted object: person: 99.41%\n",
      "predicted object: cup: 98.14%\n",
      "predicted object: person: 99.39%\n",
      "predicted object: cup: 89.55%\n",
      "predicted object: person: 98.73%\n",
      "predicted object: cup: 51.22%\n",
      "predicted object: person: 97.34%\n",
      "predicted object: cup: 81.90%\n",
      "predicted object: person: 97.37%\n",
      "predicted object: cup: 96.14%\n",
      "predicted object: person: 98.96%\n",
      "predicted object: cup: 89.04%\n",
      "predicted object: person: 98.22%\n",
      "predicted object: cup: 94.98%\n",
      "predicted object: person: 98.76%\n",
      "predicted object: cup: 91.37%\n",
      "predicted object: cup: 96.73%\n",
      "predicted object: person: 95.03%\n",
      "predicted object: person: 98.54%\n",
      "predicted object: cup: 96.35%\n",
      "predicted object: cup: 99.40%\n",
      "predicted object: person: 99.38%\n",
      "predicted object: person: 98.89%\n",
      "predicted object: cup: 98.49%\n",
      "predicted object: person: 99.33%\n",
      "predicted object: cup: 98.28%\n",
      "predicted object: cup: 99.19%\n",
      "predicted object: person: 99.02%\n",
      "predicted object: person: 99.44%\n",
      "predicted object: cup: 92.77%\n",
      "predicted object: person: 99.16%\n",
      "predicted object: cup: 61.97%\n",
      "predicted object: person: 99.69%\n",
      "predicted object: cup: 85.98%\n",
      "predicted object: person: 99.44%\n",
      "predicted object: cup: 87.83%\n",
      "predicted object: cellphone: 66.03%\n",
      "predicted object: person: 99.06%\n",
      "predicted object: cup: 93.16%\n",
      "predicted object: cup: 99.74%\n",
      "predicted object: person: 99.42%\n",
      "predicted object: person: 99.26%\n",
      "predicted object: cup: 98.33%\n",
      "predicted object: cellphone: 54.60%\n",
      "predicted object: person: 99.03%\n",
      "predicted object: cup: 98.97%\n",
      "predicted object: person: 99.46%\n",
      "predicted object: banana: 64.93%\n",
      "predicted object: person: 97.53%\n",
      "predicted object: cup: 97.29%\n",
      "predicted object: person: 99.66%\n",
      "predicted object: cup: 97.88%\n",
      "predicted object: cellphone: 97.53%\n",
      "predicted object: cup: 99.95%\n",
      "predicted object: person: 99.92%\n",
      "predicted object: cellphone: 96.74%\n",
      "predicted object: diningtable: 63.35%\n",
      "predicted object: cup: 99.94%\n",
      "predicted object: person: 99.67%\n",
      "predicted object: cellphone: 97.82%\n",
      "predicted object: person: 99.83%\n",
      "predicted object: cup: 99.58%\n",
      "predicted object: cellphone: 73.63%\n",
      "predicted object: person: 99.87%\n",
      "predicted object: cup: 99.61%\n",
      "predicted object: remote: 85.03%\n",
      "predicted object: person: 99.52%\n",
      "predicted object: cup: 99.43%\n",
      "predicted object: cellphone: 98.64%\n",
      "predicted object: cellphone: 77.72%\n",
      "predicted object: cup: 99.43%\n",
      "predicted object: person: 99.12%\n",
      "predicted object: cellphone: 97.54%\n",
      "predicted object: cellphone: 78.95%\n",
      "predicted object: cup: 99.57%\n",
      "predicted object: person: 98.54%\n",
      "predicted object: cellphone: 95.29%\n",
      "predicted object: cellphone: 82.26%\n",
      "predicted object: cup: 99.71%\n",
      "predicted object: person: 98.29%\n",
      "predicted object: cellphone: 94.91%\n",
      "predicted object: cellphone: 90.20%\n",
      "predicted object: cup: 99.87%\n",
      "predicted object: cellphone: 99.64%\n",
      "predicted object: person: 99.49%\n",
      "predicted object: cellphone: 96.99%\n",
      "predicted object: cup: 99.75%\n",
      "predicted object: cellphone: 99.62%\n",
      "predicted object: person: 98.67%\n",
      "predicted object: cellphone: 81.66%\n",
      "predicted object: cellphone: 99.77%\n",
      "predicted object: cup: 99.48%\n",
      "predicted object: person: 99.14%\n",
      "predicted object: cellphone: 92.95%\n",
      "predicted object: cup: 99.75%\n",
      "predicted object: cellphone: 99.72%\n",
      "predicted object: person: 99.19%\n",
      "predicted object: cellphone: 60.06%\n",
      "predicted object: cup: 99.76%\n",
      "predicted object: person: 98.03%\n",
      "predicted object: cellphone: 85.00%\n",
      "predicted object: cellphone: 62.97%\n",
      "predicted object: cup: 99.67%\n",
      "predicted object: person: 98.41%\n",
      "predicted object: cellphone: 69.21%\n",
      "predicted object: cellphone: 63.51%\n",
      "predicted object: cup: 99.78%\n",
      "predicted object: person: 98.65%\n",
      "predicted object: cup: 53.28%\n",
      "predicted object: cup: 99.68%\n",
      "predicted object: person: 97.73%\n",
      "predicted object: cellphone: 96.64%\n",
      "predicted object: cellphone: 50.52%\n",
      "predicted object: cup: 99.61%\n",
      "predicted object: person: 99.56%\n",
      "predicted object: cellphone: 96.76%\n",
      "predicted object: laptop: 64.40%\n",
      "predicted object: person: 98.80%\n",
      "predicted object: cellphone: 99.25%\n",
      "predicted object: laptop: 99.07%\n",
      "predicted object: person: 98.41%\n",
      "predicted object: person: 99.12%\n",
      "predicted object: laptop: 99.12%\n",
      "predicted object: cellphone: 93.17%\n",
      "predicted object: person: 99.49%\n",
      "predicted object: laptop: 99.29%\n",
      "predicted object: person: 99.72%\n",
      "predicted object: laptop: 99.55%\n",
      "predicted object: cellphone: 53.12%\n",
      "predicted object: person: 99.30%\n",
      "predicted object: laptop: 99.27%\n",
      "predicted object: person: 99.59%\n",
      "predicted object: laptop: 99.35%\n",
      "predicted object: laptop: 97.78%\n",
      "predicted object: person: 70.10%\n",
      "predicted object: person: 98.23%\n",
      "predicted object: person: 99.42%\n",
      "predicted object: cellphone: 99.56%\n",
      "predicted object: cup: 99.46%\n",
      "predicted object: person: 92.84%\n",
      "predicted object: cellphone: 68.43%\n",
      "predicted object: cup: 99.63%\n",
      "predicted object: person: 98.45%\n",
      "predicted object: cellphone: 85.60%\n",
      "predicted object: cup: 99.69%\n",
      "predicted object: person: 99.68%\n",
      "predicted object: cellphone: 99.36%\n",
      "predicted object: cellphone: 69.49%\n",
      "predicted object: person: 99.86%\n",
      "predicted object: cup: 99.80%\n",
      "predicted object: cellphone: 69.46%\n",
      "predicted object: cellphone: 63.49%\n",
      "predicted object: person: 92.52%\n",
      "predicted object: cup: 92.29%\n",
      "predicted object: remote: 67.48%\n",
      "predicted object: cup: 99.65%\n",
      "predicted object: person: 97.81%\n",
      "predicted object: cellphone: 86.30%\n",
      "predicted object: cup: 99.74%\n",
      "predicted object: cellphone: 99.40%\n",
      "predicted object: person: 99.07%\n",
      "predicted object: cellphone: 52.05%\n",
      "predicted object: cup: 99.74%\n",
      "predicted object: person: 99.54%\n",
      "predicted object: cellphone: 97.31%\n",
      "predicted object: person: 99.21%\n",
      "predicted object: cup: 99.13%\n",
      "predicted object: cellphone: 98.37%\n",
      "predicted object: person: 99.41%\n",
      "predicted object: cup: 99.00%\n",
      "predicted object: cellphone: 97.43%\n",
      "predicted object: cup: 99.79%\n",
      "predicted object: person: 99.70%\n",
      "predicted object: cellphone: 98.60%\n",
      "predicted object: cellphone: 89.05%\n",
      "predicted object: cup: 99.82%\n",
      "predicted object: person: 99.63%\n",
      "predicted object: cellphone: 97.91%\n",
      "predicted object: cellphone: 91.11%\n",
      "predicted object: cup: 99.77%\n",
      "predicted object: person: 99.64%\n",
      "predicted object: cellphone: 97.24%\n",
      "predicted object: cellphone: 92.02%\n",
      "predicted object: cup: 99.83%\n",
      "predicted object: person: 99.71%\n",
      "predicted object: cellphone: 95.89%\n",
      "predicted object: cellphone: 80.37%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object: cup: 99.72%\n",
      "predicted object: cellphone: 98.96%\n",
      "predicted object: person: 97.76%\n",
      "predicted object: cellphone: 60.07%\n",
      "predicted object: cup: 99.73%\n",
      "predicted object: cellphone: 98.77%\n",
      "predicted object: person: 98.39%\n",
      "predicted object: cup: 99.77%\n",
      "predicted object: person: 98.30%\n",
      "predicted object: cellphone: 97.50%\n",
      "predicted object: cellphone: 60.47%\n",
      "predicted object: cup: 99.74%\n",
      "predicted object: cellphone: 98.35%\n",
      "predicted object: person: 97.40%\n",
      "predicted object: cup: 99.76%\n",
      "predicted object: cellphone: 97.22%\n",
      "predicted object: person: 96.76%\n",
      "predicted object: cup: 99.74%\n",
      "predicted object: person: 98.99%\n",
      "predicted object: cellphone: 95.37%\n",
      "predicted object: cup: 99.66%\n",
      "predicted object: person: 98.63%\n",
      "predicted object: cellphone: 98.13%\n",
      "predicted object: cup: 99.49%\n",
      "predicted object: person: 98.81%\n",
      "predicted object: cellphone: 97.65%\n",
      "predicted object: cup: 99.65%\n",
      "predicted object: cellphone: 98.01%\n",
      "predicted object: person: 97.89%\n",
      "predicted object: cup: 99.79%\n",
      "predicted object: person: 98.83%\n",
      "predicted object: cellphone: 98.54%\n",
      "predicted object: cup: 99.79%\n",
      "predicted object: person: 99.59%\n",
      "predicted object: toothbrush: 55.22%\n",
      "predicted object: cup: 99.76%\n",
      "predicted object: person: 99.75%\n",
      "predicted object: cellphone: 99.62%\n",
      "predicted object: toothbrush: 63.49%\n",
      "predicted object: cup: 99.77%\n",
      "predicted object: person: 99.63%\n",
      "predicted object: cellphone: 99.58%\n",
      "predicted object: cup: 99.71%\n",
      "predicted object: person: 99.58%\n",
      "predicted object: cellphone: 99.24%\n",
      "predicted object: cup: 99.85%\n",
      "predicted object: person: 99.58%\n",
      "predicted object: cellphone: 98.57%\n",
      "predicted object: toothbrush: 61.08%\n",
      "predicted object: person: 99.82%\n",
      "predicted object: cup: 99.70%\n",
      "predicted object: cellphone: 96.69%\n",
      "predicted object: cellphone: 89.07%\n",
      "predicted object: person: 99.87%\n",
      "predicted object: cup: 99.83%\n",
      "predicted object: cellphone: 97.57%\n",
      "predicted object: cellphone: 57.48%\n",
      "predicted object: cup: 99.81%\n",
      "predicted object: person: 99.76%\n",
      "predicted object: cellphone: 96.19%\n",
      "predicted object: cellphone: 93.50%\n",
      "predicted object: person: 99.74%\n",
      "predicted object: cup: 99.80%\n",
      "predicted object: person: 98.55%\n",
      "predicted object: person: 99.82%\n",
      "predicted object: cup: 99.78%\n",
      "predicted object: cellphone: 83.91%\n",
      "predicted object: cup: 99.85%\n",
      "predicted object: person: 99.69%\n",
      "predicted object: diningtable: 54.08%\n",
      "predicted object: cup: 99.72%\n",
      "predicted object: person: 99.51%\n",
      "predicted object: cellphone: 82.24%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame_width = frame.shape[1]\n",
    "    frame_height = frame.shape[0]\n",
    "\n",
    "    frame_blob = cv2.dnn.blobFromImage(frame, 1/255, (416,416), swapRB=True,crop=False)\n",
    "    model.setInput(frame_blob)\n",
    "    detection_layers = model.forward(output_layers)\n",
    "\n",
    "    ids_list = []\n",
    "    boxes_list = []\n",
    "    confidences_list = []\n",
    "    for detection_layer in detection_layers:\n",
    "        for object_detection in detection_layer:\n",
    "            scores = object_detection[5:]\n",
    "            predicted_id = np.argmax(scores)\n",
    "            confidence = scores[predicted_id]\n",
    "            if confidence > 0.20: #draw bounding box if confidence is higher than ..\n",
    "                label = labels[predicted_id]\n",
    "                bounding_box = object_detection[0:4] * np.array([frame_width,frame_height,frame_width,frame_height])\n",
    "                (box_center_x, box_center_y,box_width, box_height) = bounding_box.astype(\"int\")\n",
    "\n",
    "                start_x = int(box_center_x - (box_width/2))\n",
    "                start_y = int(box_center_y - (box_height/2))\n",
    "\n",
    "                ## non-maximum surpression step 2 ##\n",
    "                ids_list.append(predicted_id)\n",
    "                confidences_list.append(float(confidence))\n",
    "                boxes_list.append([start_x,start_y,int(box_width),int(box_height)])\n",
    "                ## non-maximum surpression step 2 ##\n",
    "\n",
    "    ## non-maximum surpression step 3 ##\n",
    "    max_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, 0.5, 0.4)\n",
    "    for max_id in max_ids:\n",
    "        max_class_id = max_id[0]\n",
    "        box = boxes_list[max_class_id]\n",
    "        start_x = box[0]\n",
    "        start_y = box[1]\n",
    "        box_width = box[2]\n",
    "        box_height = box[3]\n",
    "\n",
    "        predicted_id = ids_list[max_class_id]\n",
    "        label = labels[predicted_id]\n",
    "        confidence = confidences_list[max_class_id]\n",
    "    ## non-maximum surpression step 3 ##\n",
    "\n",
    "        end_x = start_x + box_width\n",
    "        end_y = start_y + box_height\n",
    "\n",
    "        box_color = colors[predicted_id]\n",
    "        box_color = [int(each) for each in box_color]\n",
    "\n",
    "        label = \"{}: {:.2f}%\".format(label,confidence*100)\n",
    "        print(\"predicted object: {}\".format(label))\n",
    "\n",
    "        cv2.rectangle(frame, (start_x, start_y),(end_x, end_y),box_color,2)\n",
    "        cv2.putText(frame, label, (start_x, start_y-10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color,1)\n",
    "    cv2.imshow(\"Detection window\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fef92f",
   "metadata": {},
   "source": [
    "### Stream and detect object on video camera (inline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7655612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2 \n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame_width = frame.shape[1]\n",
    "        frame_height = frame.shape[0]\n",
    "\n",
    "        frame_blob = cv2.dnn.blobFromImage(frame, 1/255, (416,416), swapRB=True,crop=False)\n",
    "        model.setInput(frame_blob)\n",
    "        detection_layers = model.forward(output_layers)\n",
    "\n",
    "        ids_list = []\n",
    "        boxes_list = []\n",
    "        confidences_list = []\n",
    "        for detection_layer in detection_layers:\n",
    "            for object_detection in detection_layer:\n",
    "                scores = object_detection[5:]\n",
    "                predicted_id = np.argmax(scores)\n",
    "                confidence = scores[predicted_id]\n",
    "                if confidence > 0.20: #draw bounding box if confidence is higher than ..\n",
    "                    label = labels[predicted_id]\n",
    "                    bounding_box = object_detection[0:4] * np.array([frame_width,frame_height,frame_width,frame_height])\n",
    "                    (box_center_x, box_center_y,box_width, box_height) = bounding_box.astype(\"int\")\n",
    "\n",
    "                    start_x = int(box_center_x - (box_width/2))\n",
    "                    start_y = int(box_center_y - (box_height/2))\n",
    "\n",
    "                    ## non-maximum surpression step 2 ##\n",
    "                    ids_list.append(predicted_id)\n",
    "                    confidences_list.append(float(confidence))\n",
    "                    boxes_list.append([start_x,start_y,int(box_width),int(box_height)])\n",
    "                    ## non-maximum surpression step 2 ##\n",
    "\n",
    "        ## non-maximum surpression step 3 ##\n",
    "        max_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, 0.5, 0.4)\n",
    "        for max_id in max_ids:\n",
    "            max_class_id = max_id[0]\n",
    "            box = boxes_list[max_class_id]\n",
    "            start_x = box[0]\n",
    "            start_y = box[1]\n",
    "            box_width = box[2]\n",
    "            box_height = box[3]\n",
    "\n",
    "            predicted_id = ids_list[max_class_id]\n",
    "            label = labels[predicted_id]\n",
    "            confidence = confidences_list[max_class_id]\n",
    "        ## non-maximum surpression step 3 ##\n",
    "\n",
    "            end_x = start_x + box_width\n",
    "            end_y = start_y + box_height\n",
    "\n",
    "            box_color = colors[predicted_id]\n",
    "            box_color = [int(each) for each in box_color]\n",
    "\n",
    "            label = \"{}: {:.2f}%\".format(label,confidence*100)\n",
    "            print(\"predicted object: {}\".format(label))\n",
    "\n",
    "            cv2.rectangle(frame, (start_x, start_y),(end_x, end_y),box_color,2)\n",
    "            cv2.putText(frame, label, (start_x, start_y-10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color,1)\n",
    "        #cv2.imshow(\"Detection window\", frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Converting BGR to RGB\n",
    "        display(Image.fromarray(frame))\n",
    "        clear_output(wait=True)\n",
    "        #if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            #break\n",
    "#cap.release()\n",
    "#cv2.destroyAllWindows()\n",
    "#cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
